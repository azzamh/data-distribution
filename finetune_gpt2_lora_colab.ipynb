{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb1a619",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Required Libraries\n",
    "\n",
    "Install semua library yang diperlukan untuk fine-tuning dengan LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdb01a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Semua library berhasil diinstall!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers peft datasets accelerate kagglehub torch scikit-learn pyyaml\n",
    "print(\"‚úì Semua library berhasil diinstall!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b242af",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Import Dependencies\n",
    "\n",
    "Import semua library yang diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c48c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: Tesla T4\n",
      "Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc0d6e",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Create Configuration File\n",
    "\n",
    "Buat file konfigurasi untuk mengatur semua hyperparameter. Ini memudahkan eksperimen dengan berbagai setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173e5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Konfigurasi berhasil dibuat!\n",
      "\n",
      "üìã Configuration Preview:\n",
      "dataset:\n",
      "  label_column: author_id\n",
      "  max_samples_per_class: null\n",
      "  min_samples_per_class: 10\n",
      "  random_seed: 42\n",
      "  test_split: 0.1\n",
      "  text_column: text\n",
      "  train_split: 0.8\n",
      "  val_split: 0.1\n",
      "lora:\n",
      "  bias: none\n",
      "  lora_alpha: 16\n",
      "  lora_dropout: 0.1\n",
      "  r: 8\n",
      "  target_modules:\n",
      "  - c_attn\n",
      "  task_type: SEQ_CLS\n",
      "misc:\n",
      "  logging_dir: ./logs\n",
      "  seed: 42\n",
      "model:\n",
      "  max_length: 128\n",
      "  name: gpt2\n",
      "training:\n",
      "  eval_steps: 500\n",
      "  fp16: true\n",
      "  gradient_accumulation_steps: 1\n",
      "  greater_is_better: false\n",
      "  learning_rate: 0.0003\n",
      "  load_best_model_at_end: true\n",
      "  logging_steps: 50\n",
      "  metric_for_best_model: eval_loss\n",
      "  num_train_epochs: 3\n",
      "  output_dir: ./results\n",
      "  per_device_eval_batch_size: 8\n",
      "  per_device_train_batch_size: 8\n",
      "  save_steps: 500\n",
      "  save_total_limit: 2\n",
      "  warmup_steps: 500\n",
      "  weight_decay: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    # Model Configuration\n",
    "    'model': {\n",
    "        'name': 'gpt2',  # Options: gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "        'max_length': 128\n",
    "    },\n",
    "    \n",
    "    # LoRA Configuration\n",
    "    'lora': {\n",
    "        'r': 8,  # LoRA rank (4-16 recommended)\n",
    "        'lora_alpha': 16,  # Scaling parameter (usually 2*r)\n",
    "        'lora_dropout': 0.1,\n",
    "        'target_modules': ['c_attn'],  # GPT-2 attention modules\n",
    "        'bias': 'none',\n",
    "        'task_type': 'SEQ_CLS'\n",
    "    },\n",
    "    \n",
    "    # Dataset Configuration\n",
    "    'dataset': {\n",
    "        'train_split': 0.8,\n",
    "        'val_split': 0.1,\n",
    "        'test_split': 0.1,\n",
    "        'random_seed': 42,\n",
    "        'min_samples_per_class': 100,  # Minimum tweets per author (increased for Twitter)\n",
    "        'max_classes': 50,  # Maximum number of authors to include (top N by tweet count)\n",
    "        'max_samples_per_class': None,  # None = no limit\n",
    "        'text_column': 'text',\n",
    "        'label_column': 'author_id'\n",
    "    },\n",
    "    \n",
    "    # Training Configuration\n",
    "    'training': {\n",
    "        'output_dir': './results',\n",
    "        'num_train_epochs': 3,\n",
    "        'per_device_train_batch_size': 8,\n",
    "        'per_device_eval_batch_size': 8,\n",
    "        'gradient_accumulation_steps': 1,\n",
    "        'learning_rate': 3e-4,\n",
    "        'weight_decay': 0.01,\n",
    "        'warmup_steps': 500,\n",
    "        'logging_steps': 50,\n",
    "        'eval_steps': 500,\n",
    "        'save_steps': 500,\n",
    "        'save_total_limit': 2,\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'eval_loss',\n",
    "        'greater_is_better': False,\n",
    "        'fp16': True if device == 'cuda' else False,\n",
    "    },\n",
    "    \n",
    "    # Misc\n",
    "    'misc': {\n",
    "        'seed': 42,\n",
    "        'logging_dir': './logs'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a306491",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load Configuration\n",
    "\n",
    "Load konfigurasi dari file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1bbd237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Konfigurasi loaded!\n",
      "Model: gpt2\n",
      "LoRA rank: 8\n",
      "Epochs: 3\n",
      "Learning rate: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed(config['misc']['seed'])\n",
    "\n",
    "print(\"‚úì Konfigurasi loaded!\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"LoRA rank: {config['lora']['r']}\")\n",
    "print(f\"Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(f\"Learning rate: {config['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3599c3a",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Load and Explore Twitter Support Dataset\n",
    "\n",
    "Load dataset Twitter Support dari file lokal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a809031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Twitter Support dataset...\n",
      "Loading from: /Users/azzam_hanif/Documents/04_KULIah/03_SUDI_MANDIRI/experiment/data-distribution/twcs/twcs.csv\n",
      "‚ùå Failed to load dataset.\n",
      "Error: [Errno 2] No such file or directory: '/Users/azzam_hanif/Documents/04_KULIah/03_SUDI_MANDIRI/experiment/data-distribution/twcs/twcs.csv'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/azzam_hanif/Documents/04_KULIah/03_SUDI_MANDIRI/experiment/data-distribution/twcs/twcs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-255851290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ùå Failed to load dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-255851290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/azzam_hanif/Documents/04_KULIah/03_SUDI_MANDIRI/experiment/data-distribution/twcs/twcs.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading from: {csv_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset loaded with {len(df)} samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/azzam_hanif/Documents/04_KULIah/03_SUDI_MANDIRI/experiment/data-distribution/twcs/twcs.csv'"
     ]
    }
   ],
   "source": [
    "# Load Twitter Support dataset from local CSV\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üì• Loading Twitter Support dataset...\")\n",
    "try:\n",
    "    # Load from local CSV file\n",
    "    csv_file = \"/Users/azzam_hanif/Documents/04_KULIah/03_SUDI_MANDIRI/experiment/data-distribution/twcs/twcs.csv\"\n",
    "    print(f\"Loading from: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Dataset loaded with {len(df)} samples\")\n",
    "\n",
    "    # Use 'author_id' as label (predicting which company/author based on tweet text)\n",
    "    # Keep only text and author_id columns\n",
    "    df = df[['text', 'author_id']].dropna()\n",
    "    \n",
    "    print(\"‚úì Dataset loaded successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to load dataset.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    raise e\n",
    "\n",
    "print(\"shape\", df.shape)\n",
    "print(\"First 5 records:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee33e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Sample Data:\n",
      "                                                text  author_id\n",
      "0  Stuning even for the non-gamer: This sound tra...          1\n",
      "1  The best soundtrack ever to anything.: I'm rea...          1\n",
      "2  Amazing!: This soundtrack is my favorite music...          1\n",
      "3  Excellent Soundtrack: I truly like this soundt...          1\n",
      "4  Remember, Pull Your Jaw Off The Floor After He...          1\n",
      "5  an absolute masterpiece: I am quite sure any o...          1\n",
      "6  Buyer beware: This is a self-published book, a...          0\n",
      "7  Glorious story: I loved Whisper of the wicked ...          1\n",
      "8  A FIVE STAR BOOK: I just finished reading Whis...          1\n",
      "9  Whispers of the Wicked Saints: This was a easy...          1\n",
      "\n",
      "‚úì Amazon Reviews dataset detected (binary sentiment classification)\n",
      "\n",
      "======================================================================\n",
      "DATASET DISTRIBUTION\n",
      "======================================================================\n",
      "Total unique authors/airlines: 2\n",
      "Total tweets: 3600000\n",
      "\n",
      "All authors/airlines:\n",
      " 1. 1                             : 1800000 tweets\n",
      " 2. 0                             : 1800000 tweets\n",
      "\n",
      "======================================================================\n",
      "DATASET DISTRIBUTION\n",
      "======================================================================\n",
      "Total unique authors/airlines: 2\n",
      "Total tweets: 3600000\n",
      "\n",
      "All authors/airlines:\n",
      " 1. 1                             : 1800000 tweets\n",
      " 2. 0                             : 1800000 tweets\n"
     ]
    }
   ],
   "source": [
    "# Display sample data\n",
    "print(\"üìù Sample Data:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# For Twitter Support dataset (multi-class author classification)\n",
    "print(\"\\n‚úì Twitter Support dataset detected (author classification)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Twitter authors\n",
    "author_counts = df['author_id'].value_counts()\n",
    "print(f\"Total unique authors: {len(author_counts)}\")\n",
    "print(f\"Total tweets: {len(df)}\")\n",
    "print(f\"\\nAuthor distribution (top 20):\")\n",
    "for i, (author, count) in enumerate(author_counts.head(20).items(), 1):\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{i:2d}. {str(author):30s}: {count:6d} tweets ({percentage:5.2f}%)\")\n",
    "if len(author_counts) > 20:\n",
    "    print(f\"     ... and {len(author_counts) - 20} more authors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a04fe",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Preprocess Dataset\n",
    "\n",
    "Preprocess dataset: filter, create labels, dan split train/val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94eff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 14640\n",
      "After removing NaN: 14640\n",
      "After filtering (min 10 samples): 14640\n",
      "\n",
      "‚úì Preprocessing complete!\n",
      "Number of classes: 6\n",
      "Final dataset size: 14640\n"
     ]
    }
   ],
   "source": [
    "text_col = config['dataset']['text_column']\n",
    "label_col = config['dataset']['label_column']\n",
    "\n",
    "# Remove missing values\n",
    "print(f\"Original size: {len(df)}\")\n",
    "df = df[[text_col, label_col]].dropna()\n",
    "print(f\"After removing NaN: {len(df)}\")\n",
    "\n",
    "# Filter by minimum samples per class\n",
    "min_samples = config['dataset']['min_samples_per_class']\n",
    "author_counts = df[label_col].value_counts()\n",
    "valid_authors = author_counts[author_counts >= min_samples].index\n",
    "df = df[df[label_col].isin(valid_authors)]\n",
    "print(f\"After filtering (min {min_samples} samples): {len(df)}\")\n",
    "\n",
    "# Select top N authors by tweet count\n",
    "max_classes = config['dataset']['max_classes']\n",
    "if max_classes and len(author_counts) > max_classes:\n",
    "    top_authors = author_counts.head(max_classes).index\n",
    "    df = df[df[label_col].isin(top_authors)]\n",
    "    print(f\"After selecting top {max_classes} authors: {len(df)}\")\n",
    "\n",
    "# Create label mapping\n",
    "unique_authors = sorted(df[label_col].unique())\n",
    "label2id = {author: idx for idx, author in enumerate(unique_authors)}\n",
    "id2label = {idx: author for author, idx in label2id.items()}\n",
    "\n",
    "df['label'] = df[label_col].map(label2id)\n",
    "\n",
    "print(f\"\\n‚úì Preprocessing complete!\")\n",
    "print(f\"Number of classes: {len(unique_authors)}\")\n",
    "print(f\"Final dataset size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53074879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET SPLITS\n",
      "======================================================================\n",
      "Train: 11712 samples (80.0%)\n",
      "Val: 1464 samples (10.0%)\n",
      "Test: 1464 samples (10.0%)\n",
      "\n",
      "‚úì Dataset split berhasil!\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "train_split = config['dataset']['train_split']\n",
    "val_split = config['dataset']['val_split']\n",
    "test_split = config['dataset']['test_split']\n",
    "seed = config['dataset']['random_seed']\n",
    "\n",
    "# Train vs (Val + Test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=(val_split + test_split),\n",
    "    random_state=seed,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Val vs Test\n",
    "val_ratio = val_split / (val_split + test_split)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=(1 - val_ratio),\n",
    "    random_state=seed,\n",
    "    stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATASET SPLITS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Train: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Val: {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "print(\"\\n‚úì Dataset split berhasil!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41544eb4",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Load GPT-2 Model with LoRA\n",
    "\n",
    "Load base GPT-2 model untuk sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d83662a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading gpt2 model...\n",
      "Number of labels (authors): 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4634f10fd084d82aae8254a4a005f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048c0cb7f6084be0a9b13c6cdcc0487d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded: gpt2\n",
      "Total parameters: 124,444,416\n"
     ]
    }
   ],
   "source": [
    "model_name = config['model']['name']\n",
    "num_labels = len(label2id)\n",
    "\n",
    "print(f\"üì¶ Loading {model_name} model...\")\n",
    "print(f\"Number of labels (authors): {num_labels}\")\n",
    "\n",
    "# Load model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model loaded: {model_name}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf775a39",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Configure LoRA Parameters\n",
    "\n",
    "Aplikasikan LoRA configuration ke model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a5992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LoRA applied to model!\n",
      "\n",
      "======================================================================\n",
      "LoRA Configuration\n",
      "======================================================================\n",
      "Rank (r): 8\n",
      "Alpha: 16\n",
      "Dropout: 0.1\n",
      "Target modules: ['c_attn']\n",
      "trainable params: 299,520 || all params: 124,743,936 || trainable%: 0.2401\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=config['lora']['r'],\n",
    "    lora_alpha=config['lora']['lora_alpha'],\n",
    "    lora_dropout=config['lora']['lora_dropout'],\n",
    "    target_modules=config['lora']['target_modules'],\n",
    "    bias=config['lora']['bias'],\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úì LoRA applied to model!\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"LoRA Configuration\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Rank (r): {config['lora']['r']}\")\n",
    "print(f\"Alpha: {config['lora']['lora_alpha']}\")\n",
    "print(f\"Dropout: {config['lora']['lora_dropout']}\")\n",
    "print(f\"Target modules: {config['lora']['target_modules']}\")\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f5ee6",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Prepare Tokenizer and Data Collator\n",
    "\n",
    "Setup tokenizer dan data collator untuk preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23abd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a47bd3000142beaf4e9d537a9f20ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8ca3b2ea0c4a35bd16a27920288a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da33264393545778c3d749d0546bbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf31f5e626b422593389d63fa9e4bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3765ece88264074b18089257ee350e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11712 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33df712f951a4302ae7cf210965af394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83c3d53cae54301afb62401cd78b8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have pad token, so we add one\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=config['model']['max_length'],\n",
    "        padding=False  # Will be handled by data collator\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"üîÑ Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=['text'])\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=['text'])\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"‚úì Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b98d20",
   "metadata": {},
   "source": [
    "## üîü Setup Training Arguments\n",
    "\n",
    "Configure training arguments dari config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88044f61",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1533079532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Setup training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_train_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config['training']['output_dir'],\n",
    "    num_train_epochs=config['training']['num_train_epochs'],\n",
    "    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    learning_rate=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay'],\n",
    "    warmup_steps=config['training']['warmup_steps'],\n",
    "    logging_dir=config['misc']['logging_dir'],\n",
    "    logging_steps=config['training']['logging_steps'],\n",
    "    eval_strategy='steps',  # Updated from evaluation_strategy\n",
    "    eval_steps=config['training']['eval_steps'],\n",
    "    save_steps=config['training']['save_steps'],\n",
    "    save_total_limit=config['training']['save_total_limit'],\n",
    "    load_best_model_at_end=config['training']['load_best_model_at_end'],\n",
    "    metric_for_best_model=config['training']['metric_for_best_model'],\n",
    "    greater_is_better=config['training']['greater_is_better'],\n",
    "    fp16=config['training']['fp16'],\n",
    "    report_to='none',\n",
    "    seed=config['misc']['seed'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fead52",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Initialize Trainer\n",
    "\n",
    "Setup Trainer dengan model, data, dan training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52c01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Trainer initialized!\n",
      "Ready to train on 10 samples\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized!\")\n",
    "print(f\"Ready to train on {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657bc64d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Start Training\n",
    "\n",
    "Mulai proses fine-tuning model dengan LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"Training on {len(train_dataset)} samples\")\n",
    "print(f\"Validating on {len(val_dataset)} samples\")\n",
    "print(f\"Model: {model_name} with LoRA (r={config['lora']['r']})\")\n",
    "print(f\"Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n‚úì Training completed!\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final train loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./final_model\")\n",
    "print(\"‚úì Model saved to ./final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa179a8",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Evaluate Model\n",
    "\n",
    "Evaluasi performa model pada test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üìä Evaluating model on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['eval_recall']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Loss: {test_results['eval_loss']:.4f}\")\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "report = classification_report(labels, preds, target_names=list(id2label.values())[:10], zero_division=0)\n",
    "print(report)\n",
    "\n",
    "print(\"‚úì Evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
